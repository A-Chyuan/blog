# Introduction

Concurrency in software execution can occur at four different levels:

- **Instruction level**: Executing two or more *machine instructions* simultaneously.
- **Statement level**: Executing two or more *high-level language statements* simultaneously.
- **Unit level**: Executing two or more *subprogram units* simultaneously.
- **Program level**: Executing two or more *programs* simultaneously.

?> Concurrency at both the *subprogram* and the *statement levels* is discussed in this chapter, with most of the focus on the subprogram level.

The intention of this chapter is to discuss the aspects of concurrency that are most relevant to language design issues, rather than to present a definitive study of all of the issues of concurrency, including the development of concurrent programs. That would clearly be inappropriate for a book on programming languages.

***Challenges and Applications***

At first glance, concurrency may appear to be a simple concept, but it presents significant challenges to the programmer, the programming language designer, and the operating system designer (because much of the support for concurrency is provided by the operating system).

Concurrent control mechanisms increase programming flexibility.

- They were originally invented to be used for particular problems faced in operating systems, but they are required for a variety of other programming applications.
- One of the most commonly used programs is Web browsers, whose design is based heavily on concurrency.

    Browsers must perform many different functions at the same time, among them sending and receiving data from Web servers, rendering text and images on the screen, and reacting to user actions with the mouse and the keyboard.

    Most contemporary browsers use the extra core processors that are part of many contemporary personal computers to perform some of their processing, for example the interpretation of client-side scripting code.

- Another example is the software systems that are designed to simulate actual physical systems that consist of multiple concurrent subsystems.
- For all of these kinds of applications, the programming language (or a library or at least the operating system) must support unit-level concurrency.

***Statement-Level Concurrency***

Statement-level concurrency is quite different from concurrency at the unit level. From a language designer’s point of view, statement-level concurrency is largely a matter of specifying

- how data should be distributed over multiple memories.
- which statements can be executed concurrently.

***Concurrent Algorithms***

The goal of developing concurrent software is to produce scalable and portable concurrent algorithms.

- A concurrent algorithm is **scalable** if the speed of its execution increases when more processors are available. This is important because the number of processors sometimes increases with the new generations of machines.
- The algorithms must be **portable** because the lifetime of hardware is relatively short. Therefore, software systems should not depend on a particular architecture—that is, they should run efficiently on machines with different architectures.

## Multiprocessor Architectures

***Early: Peripheral Processors***

The first computers that had multiple processors had one general-purpose processor and one or more other processors, often called peripheral processors, that were used <mark>only for input and output operations</mark>.

This architecture allowed those computers, which appeared in the late 1950s, to execute one program while concurrently performing input or output for that program or other programs.

***Early: Batch Scheduling***

By the early 1960s, there were machines that had multiple complete processors.

- These processors were used by the job scheduler of the operating system, which distributed separate jobs from a batch-job queue to the separate processors.
- Systems with this structure supported *program-level concurrency*.

***Early: Partial Processors***

In the mid-1960s, some machines appeared that had several identical partial processors. These were fed instructions from a single instruction stream.

- For example, some machines had two or more floating-point multipliers, while others had two or more complete floating-point arithmetic units.
- The compilers for these machines were required to determine which instructions could be executed concurrently and to schedule these instructions accordingly.
- Systems with this structure supported *instruction-level concurrency*.

***Instruction and Data Streams***

In 1966, Michael J. Flynn suggested a categorization of computer architectures defined by whether the instruction and data streams were single or multiple.

The names of these were widely used from the 1970s to the early 2000s. The two categories that used multiple data streams are defined as follows:

- Computers that have multiple processors that <mark>execute the same instruction simultaneously, each on different data</mark>, are called Single Instruction, Multiple Data (**SIMD**) architecture computers.

    In an SIMD computer, each processor has its own local memory. One processor controls the operation of the other processors. Because all of the processors, except the controller, execute the same instruction at the same time, no synchronization is required in the software.

  <div class="alert-example">

    Perhaps the most widely used SIMD machines are a category of machines called **vector processors**.

    - They have groups of registers that store the operands of a vector operation in which the same instruction is executed on the whole group of operands simultaneously.
    - Originally, the kinds of programs that could most benefit from this architecture were in scientific computation, an area of computing that is often the target of multiprocessor machines.
    - However, SIMD processors are now used for a variety of application areas, among them graphics and video processing.
    - Until recently, most supercomputers were vector processors.

  </div>

- Computers that have multiple processors that operate independently but whose operations can be synchronized are called Multiple Instruction, Multiple Data (**MIMD**) computers. <mark>Each processor in an MIMD computer executes
its own instruction stream.</mark>

  MIMD computers can appear in two distinct configurations: distributed and shared memory systems.

    - The distributed MIMD machines, in which each processor has its own memory, can be either built in a single chassis or distributed, perhaps over a large area.
    - The shared-memory MIMD machines obviously must provide some means of synchronization to prevent memory access clashes.

  Even distributed MIMD machines require synchronization to operate together on single programs.

MIMD computers, which are more general than SIMD computers, support unit-level concurrency.

?> The primary focus of this chapter is on language design for *shared memory MIMD* computers, which are often called **multiprocessors**.

---

With the advent of powerful but low-cost single-chip computers, it became possible to have large numbers of these microprocessors *connected* into physically small networks within a single chassis. These kinds of computers, which often use off-the-shelf microprocessors, are available from a number of different manufacturers.

***Hidden Concurrency***

One important reason why software has not evolved faster to make use of concurrent machines is that the power of processors has continually increased.

One of the strongest motivations to use concurrent machines is to increase the speed of computation. However, two hardware factors have combined to provide faster computation, without requiring any change in the architecture of software systems.

- First, processor clock rates have become faster with each new generation of processors (the generations have appeared roughly every 18 months).
- Second, several different kinds of concurrency have been built into the processor architectures.

  Among these are

    - the pipelining of instructions and data from the memory to the processor (instructions are fetched and decoded for future execution while the current instruction is being executed),
    - the use of separate lines for instructions and data,
    - prefetching of instructions and data,
    - and parallelism in the execution of arithmetic operations.

  All of these are collectively called **hidden concurrency**.

***Modern Challenges***

The result of the increases in execution speed is that there have been great productivity gains without requiring software developers to produce concurrent software systems.

However, the situation has changed.

- The *end* of the sequence of significant increases in the speed of individual processors is now near.
- Significant increases in computing power now result from <mark>increases in the number of processors</mark>, for example large server systems like those run by Google and Amazon and scientific research applications. Many other large computing tasks are now run on machines with large numbers of relatively small processors.

Another recent advance in computing hardware was the development of <mark>multiple processors on a single chip</mark>, such as with the Intel Core Duo and Core Quad chips, which is putting more pressure on software developers to make more use of the available multiple processor machines. If they do not, the concurrent hardware will be wasted and significant productivity gains will not be realized.

## Categories of Concurrency

There are two distinct categories of concurrent unit control.

- The most natural category of concurrency is that in which, assuming that more than one processor is available, several program units from the same program literally execute simultaneously. This is **physical concurrency**.

- A slight relaxation of this concept of concurrency allows the programmer and the application software to assume that there are multiple processors providing actual concurrency, when in fact the actual execution of programs is taking place in *interleaved fashion* on a single processor. This is **logical concurrency**.

From the programmer’s and language designer’s points of view, logical concurrency is the same as physical concurrency. It is the language implementor’s task, using the capabilities of the underlying operating system, to map the logical concurrency to the host hardware.

Both logical and physical concurrencies allow the concept of concurrency to be used as a program design methodology. For the remainder of this chapter, the discussion will apply to both physical and logical concurrencies.

***Thread of Control***

One useful technique for visualizing the flow of execution through a program is to imagine a thread laid on the statements of the source text of the program.

- Every statement reached on a particular execution is covered by the thread representing that execution.
- Visually following the thread through the source program traces the execution flow through the executable version of the program.
- Of course, in all but the simplest of programs, the thread follows a highly complex path that would be impossible to follow visually.

Formally, a **thread of control** in a program is the sequence of program points reached as control flows through the program.

Programs that have coroutines (see Chapter 9) but no concurrent subprograms, though they are sometimes called **quasi-concurrent**, have a single thread of control.

- Programs executed with physical concurrency can have multiple threads of control. Each processor can execute one of the threads.
- Although logically concurrent program execution may actually have only a single thread of control, such programs can be designed and analyzed only by imagining them as having multiple threads of control.

    A program designed to have more than one thread of control is said to be **multithreaded**.

    When a multithreaded program executes on a single-processor machine, its threads are mapped onto a single thread. It becomes, in this scenario, a virtually multithreaded program.

***Statement-Level Concurrency***

Statement-level concurrency is a relatively simple concept.

In a common use of statement-level concurrency, loops that include statements that operate on array elements are unwound so that the processing can be distributed over multiple processors.

<div class="alert-example">

A loop that executes 500 repetitions and includes a statement that operates on one of 500 array elements may be unwound so that each of 10 different processors can simultaneously process 50 of the array elements.

</div>

## Motivations for the Use of Concurrency

There are at least four different reasons to design concurrent software systems.

- The first reason is the speed of execution of programs on machines with multiple processors. These machines provide an effective way of increasing the execution speed of programs, provided that the programs are designed to make use of the concurrent hardware.
- The second reason is that even when a machine has just one processor, a program written to use concurrent execution can be faster than the same program written for sequential (nonconcurrent) execution.

    The requirement for this to happen is that the program is not compute bound (the sequential version does not fully utilize the processor).

- The third reason is that concurrency provides a different method of conceptualizing program solutions to problems.

    - Many problem domains lend themselves naturally to concurrency in much the same way that recursion is a natural way to design solutions to some problems.
    - Also, many programs are written to simulate physical entities and activities.

        In many cases, the system being simulated includes more than one entity, and the entities do whatever they do simultaneously.

      <div class="alert-example">

        Software that uses concurrency must be used to simulate such systems accurately.

        - Aircraft flying in a controlled airspace
        - Relay stations in a communications network
        - The various machines in a factory

      </div>

- The fourth reason for using concurrency is to program applications that are distributed over several machines, either locally or through the Internet.

  <div class="alert-example">

    Many machines, for example cars, have more than one built-in computer, each of which is dedicated to some specific task. In many cases, these collections of computers must synchronize their program executions.

  </div>

  <div class="alert-example">

    Internet games are another example of software that is distributed over multiple processors.

  </div>

Concurrency is now used in numerous everyday computing tasks. In short, concurrency has become a ubiquitous part of computing.

<div class="alert-example">

- Web servers process document requests concurrently.
- Web browsers now use secondary core processors to run graphic processing and to interpret programming code embedded in documents.
- In every operating system there are many concurrent processes being executed at all times, managing resources, getting input from keyboards, displaying output from programs, and reading and writing external memory devices.

</div>
