# Introduction to Subprogram-Level Concurrency

Before language support for concurrency can be considered, one must understand the underlying concepts of concurrency and the requirements for it to be useful. These topics are covered in this section.

## Fundamental Concepts

***Definition of Task***

A **task** is a unit of a program, similar to a subprogram, that can be in concurrent execution with other units of the same program.

- Each task in a program can support one thread of control.
- Tasks are sometimes called **processes**.
- In some languages, for example Java and C#, certain methods serve as tasks. Such methods are executed in objects called **threads**.

***Task vs. Subprogram***

Three characteristics of tasks distinguish them from subprograms.

- First, a task may be implicitly started, whereas a subprogram must be explicitly called.
- Second, when a program unit invokes a task, in some cases it need not wait for the task to complete its execution before continuing its own.
- Third, when the execution of a task is completed, control may or may not return to the unit that started that execution.

***Two Categories***

Tasks fall into two general categories:

- A **heavyweight task** executes in its *own* address space.
- **Lightweight tasks** all run in the *same* address space.

It is easier to implement lightweight tasks than heavyweight tasks. Furthermore, lightweight tasks can be more efficient than heavyweight tasks, because less effort is required to manage their execution.

***Communication Between Tasks***

A task can communicate with other tasks through

- Shared nonlocal variables
- Message passing
- Parameters

If a task does not communicate with or affect the execution of any other task in the program in any way, it is said to be **disjoint**. Because tasks often work together to create simulations or solve problems and therefore are not disjoint, they must use some form of communication to either synchronize their executions or share data or both.

***Synchronization***

**Synchronization** is a mechanism that controls the order in which tasks execute. Two kinds of synchronization are required when tasks share data:

- **Cooperation synchronization** is required between task A and task B <mark>when task A must wait for task B to complete some specific activity</mark> before task A can begin or continue its execution.
- **Competition synchronization** is required between two tasks when <mark>both require the use of some resource that cannot be simultaneously used.</mark> Specifically, if task A needs to access shared data location x while task B is accessing x, task A must wait for task B to complete its processing of x.

So,

- for *cooperation* synchronization, tasks may need to *wait for* the completion of specific *processing* on which their correct operation depends,
- whereas for *competition* synchronization, tasks may need to *wait for* the completion of any other processing by any task currently occurring on specific *shared data*.

<div class="alert-example">

A simple form of *cooperation* synchronization can be illustrated by a common problem called the **producer-consumer problem**.

This problem originated in the development of operating systems, in which one program unit produces some data value or resource and another uses it.

- Produced data are usually placed in a storage buffer by the producing unit and removed from that buffer by the consuming unit.
- The sequence of stores to and removals from the buffer must be synchronized.
- The consumer unit must not be allowed to take data from the buffer if the buffer is empty.
- Likewise, the producer unit cannot be allowed to place new data in the buffer if the buffer is full.

This is a problem of cooperation synchronization because the users of the shared data structure must cooperate if the buffer is to be used correctly.

</div>

Competition synchronization prevents two tasks from accessing a shared data structure at exactly the same time—a situation that could destroy the integrity of that shared data. To provide competition synchronization, *mutually exclusive access* to the shared data must be guaranteed.

<div class="alert-example">

To clarify the competition problem, consider the following scenario:

Suppose

- Task `A` has the statement `TOTAL += 1`, where `TOTAL` is a shared integer variable.
- Task `B` has the statement `TOTAL *= 2`.
- Task `A` and task `B` could try to change `TOTAL` at the same time.

At the machine language level, each task may accomplish its operation on
`TOTAL` with the following three-step process:

1. Fetch the value of `TOTAL`.
2. Perform the arithmetic operation.
3. Put the new value back in `TOTAL`.

*Without* competition synchronization, given the previously described operations performed by tasks `A` and `B` on `TOTAL`, four different values could result, depending on the order of the steps of the operations.

Assume `TOTAL` has the value 3 before either `A` or `B` attempts to modify it.

- If task `A` completes its operation before task `B` begins, the value will be 8, which is assumed here to be correct. (`B` fetch 到的值為 4)
- If `B` completes its operation before task `A` begins, the value will be 7. (`A` fetch 到的值為 6)

But if both `A` and `B` fetch the value of `TOTAL` before either task puts its new value back, the result will be incorrect. (不管 `A` 或 `B` fetch 到的值皆為 3)

- If `A` puts its value back first, the value of `TOTAL` will be 6. This case is shown in Figure 13.1.

    ![](./img/13.1.jpg ':figure The need for competition synchronization.')

- If `B` puts its value back first, the value of `TOTAL` will be 4.

A situation that leads to these problems is sometimes called a **race condition**, because two or more tasks are racing to use the shared resource and the behavior of the program depends on which task arrives first (and wins the race).

The importance of competition synchronization should now be clear.

</div>

***Mutually Exclusive Access***

One general method for providing mutually exclusive access (to support *competition synchronization*) to a shared resource is to consider the resource to be something that a task can possess and allow only a single task to possess it at a time.

- To gain possession of a shared resource, a task must request it.
- Possession will be granted only when no other task has possession.
- While a task possesses a resource, all other tasks are prevented from having access to that resource.
- When a task is finished with a shared resource that it possesses, it must relinquish that resource so it can be made available to other tasks.

Three methods of providing for mutually exclusive access to a shared resource are

- **Semaphores**, which are discussed in [Section 13-3][];
- **Monitors**, which are discussed in [Section 13-4][];
- **Message** passing, which is discussed in [Section 13-5][].

[Section 13-3]: /notes/programming-language/程式語言概念/ch13/13-3
[Section 13-4]: /notes/programming-language/程式語言概念/ch13/13-4
[Section 13-5]: /notes/programming-language/程式語言概念/ch13/13-5

***Scheduler***

Mechanisms for synchronization must be able to delay task execution. Synchronization imposes an order of execution on tasks that is enforced with these delays.

To understand what happens to tasks through their lifetimes, we must consider how task execution is controlled.

- Regardless of whether a machine has a single processor or more than one, there is always the possibility of there being more tasks than there are processors.

A run-time system program called a **scheduler** manages the sharing of processors among the tasks.

- If there were never any interruptions and tasks all had the same priority, the scheduler could simply give each  ask a time slice, such as 0.1 second, and when a task’s turn came, the scheduler could let it execute on a processor for that amount of time.
- Of course, there are several events that complicate this:

  <div class="alert-example">

    Task delays for synchronization and for input or output operations.

    - Because input and output operations are very *slow* relative to the processor’s speed,
    - a task is not allowed to keep a processor while it waits for completion of such an operation.

  </div>

***Task States***

Tasks can be in several different states:

1. *New*: A task is in the new state when it has been created but has not yet begun its execution.
2. *Ready*: A ready task is ready to run but is not currently running. Either it has not been given processor time by the scheduler, or it had run previously but was blocked in one of the ways described in Paragraph 4 of this subsection. Tasks that are ready to run are stored in a queue that is often called the **task-ready queue**.
3. *Running*: A running task is one that is currently executing; that is, it has a processor and its code is being executed.
4. *Blocked*: A task that is blocked has been running, but that execution was interrupted by one of several different events, the most common of which is an input or output operation. In addition to input and output, some languages provide operations for the user program to specify that a task be blocked.
5. *Dead*: A dead task is no longer active in any sense. A task dies when its execution is completed or it is explicitly killed by the program.

A flow diagram of the states of a task is shown in Figure 13.2.

![](./img/13.2.jpg ':figure Flow diagram of task states.')

One important issue in task execution is the following:

How is a ready task chosen to move to the running state when the task currently running has become blocked or whose time slice has expired?

- Several different algorithms have been used for making this choice, some based on specifiable priority levels.
- The algorithm that does the choosing is implemented in the scheduler.

***Liveness and Deadlock***

Associated with the concurrent execution of tasks and the use of shared resources is the concept of liveness. In the environment of sequential programs,

?> a program has the **liveness** characteristic <mark>if it continues to execute, eventually leading to completion</mark>.

- In more general terms, liveness means that if some event (e.g. program completion) is supposed to occur, it will occur, eventually. That is, progress is continually made.

In concurrent environments with shared resources, liveness can *fail*, leading to situations where the program cannot continue and will never terminate.

<div class="alert-example">

Suppose

- task A and task B both need the shared resources X and Y to complete their work.
- task A gains possession of X and task B gains possession of Y.

After some execution, task A needs resource Y to continue, so it requests Y but must wait until B releases it. Likewise, task B requests X but must wait until A releases it.

Neither relinquishes the resource it possesses, and as a result, both lose their liveness, guaranteeing that execution of the program will never complete normally. This particular kind of *loss of liveness* is called **deadlock**.

Deadlock is a serious threat to the reliability
of a program, and therefore its avoidance demands serious consideration in
both language and program design.

</div>

We are now ready to discuss some of the linguistic mechanisms for providing
concurrent unit control.

## Design Issues

The most important design issues for language support for concurrency have already been discussed at length: competition and cooperation synchronization.

In addition to these, there are several design issues of secondary importance.

- Prominent among them is how an application can influence task scheduling.
- Also, there are the issues of how and when tasks start and end their executions, and how and when they are created.

The following sections discuss three alternative approaches to the design issues for concurrency:

- Semaphores
- Monitors
- Message passing
