# Algorithms as a technology

We should choose algorithms that use the resources of time and space efficiently.

## Efficiency

Different algorithms devised to solve the same problem often differ dramatically in their efficiency. These differences can be much more significant than differences due to hardware and software.

<div class="alert-example">

Chapter 2 introduces two algorithms for sorting.

- The first, known as **insertion sort**, takes time roughly equal to $c_1 \cdot n^2$ to sort $n$ items, where $c_1$ is a constant that does not depend on $n$. That is, it takes time roughly proportional to $n^2$.
- The second, **merge sort**, takes time roughly equal to $c_2 \cdot n \log n$, where $\log n$ stands for $log_2^n$ and $c_2$ is another constant that also does not depend on $n$.

Insertion sort typically has a smaller constant factor than merge sort, so that $c_1 < c_2$ . We’ll see that the constant factors can have *far less* of an impact on the running time than the dependence on the input size $n$.

Let’s write

- Insertion sort’s running time as $c_1 \cdot n \cdot n$
- Merge sort’s running time as $c_2 \cdot n \cdot \log n$

Then we see that where insertion sort has a factor of $n$ in its running time, merge sort has a factor of $\log n$, which is much smaller.

- For example, when $n$ is 1000, $\log n$ is approximately 10, and when $n$ is 1,000,000, $\log n$ is approximately only 20.

Although insertion sort usually runs faster than merge sort for small input sizes, once the input size $n$ becomes large enough, merge sort’s advantage of $\log n$ versus $n$ more than compensates for the difference in constant factors. No matter how much smaller $c_1$ is than $c_2$ , there is always a crossover point beyond which merge sort is faster.

</div>
