# Analyzing Algorithms

## Random-Access Machine

Most of this book assumes a generic one-processor, **random-access machine (RAM)** model of computation as the implementation technology, with the understanding that algorithms are implemented as computer programs.

In the RAM model:

- Instructions execute one after another, with no concurrent operations.
- Each instruction takes the same amount of time as any other instruction.
- Each data access—using the value of a variable or storing into a variable—takes the same amount of time as any other data access.
- In other words, in the RAM model each instruction or data access takes a constant amount of time—even indexing into an array.

## Analysis of Insertion Sort

We can determine how long it takes by analyzing the algorithm itself. We’ll examine how many times it executes each line of pseudocode and how long each line of pseudocode takes to run.

Even though the running time can depend on many features of the input, we’ll focus on the one that has been shown to have the greatest effect, namely the *size of the input*, and <mark>describe the running time of a program as a function of the size of its input.</mark>

- To do so, we need to define the terms “running time” and “input size” more carefully.
- We also need to be clear about whether we are discussing the running time for an input that elicits the worst-case behavior, the best-case behavior, or some other case.

***Input Size***

The best notion for **input size** depends on the problem being studied. We’ll indicate which input size measure is being used with each problem we study.

<div class="alert-example">

- For many problems, such as sorting or computing discrete Fourier transforms, the most natural measure is the number of items in the input—for example, the number n of items being sorted.
- For many other problems, such as multiplying two integers, the best measure of input size is the total number of bits needed to represent the input in ordinary binary notation.
- Sometimes it is more appropriate to describe the size of the input with more than just one number. For example, if the input to an algorithm is a graph, we usually characterize the input size by both the number of vertices and the number of edges in the graph.

</div>

***Running Time***

The **running time** of an algorithm on a particular input is the sum of the running times of each instruction and data access executed. How we account for these costs should be independent of any particular computer, but within the framework of the RAM model.

For the moment, let us adopt the following view.

- A constant amount of time is required to execute each line of our pseudocode.
- One line might take more or less time than another line, but we’ll assume that each execution of the $k$ th line takes $c_k$ time, where $c_k$ is a constant.

---

To analyze the insertion-sort procedure, let’s view it on the following page with the time cost of each statement and the number of times each statement is executed.

- For each $i = 2, 3, \dots, n$, let $t_i$ denote the number of times the while loop test in line 6 is executed for that value of $i$.
- When a $\textbf{for}$ or $\textbf{while}$ loop exits in the usual way—because the test in the loop header comes up FALSE—the test is executed one time more than the loop body.
- Because comments are not executable statements, assume that they take no time.

<div class="table-wrapper">
<table>

<thead>
<tr>
<th>Pseudocode</th>
<th>Cost</th>
<th>Times</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">

```algorithm
\begin{algorithm}
\begin{algorithmic}
\procedure{insertion-sort}{A, n}
    \for{i = 2 to n}
        \state key = A[i]
        \state \comment{Insert A[i] into the sorted subarray A[1 : i $-$ 1].}
        \state j = i $-$ 1
        \while{j > 0 and A[j] > key}
            \state A[j + 1] = A[j]
            \state j = j $-$ 1
        \endWhile
        \state A[j + 1] = key
    \endFor
\endProcedure
\end{algorithmic}
\end{algorithm}
```

</td>
<td class="no-line-num" align="left">

```algorithm
\begin{algorithm}
\begin{algorithmic}
\state -
\state $C_2$
\state $C_3$
\state 0
\state $C_5$
\state $C_6$
\state $C_7$
\state $C_8$
\state -
\state $C_{10}$
\state -
\state -
\end{algorithmic}
\end{algorithm}
```

</td>
<td class="no-line-num" align="left">

```algorithm
\begin{algorithm}
\begin{algorithmic}
\state -
\state $n$
\state $n - 1$
\state $n - 1$
\state $n - 1$
\state $\sum_{i=2}^{n} t_i$
\state $\sum_{i=2}^{n} (t_i - 1)$
\state $\sum_{i=2}^{n} (t_i - 1)$
\state -
\state $n - 1$
\state -
\state -
\end{algorithmic}
\end{algorithm}
```

</td>
</tr>
</tbody>
</table>
</div>

The running time of the algorithm is the sum of running times for each statement executed.

- A statement that takes $c_k$ time to execute and executes $m$ times contributes $c_k m$ to the total running time.
- We usually denote the running time of an algorithm on an input of size $n$ by $T(n)$.
- To compute $T(n)$, the running time of insertion-sort on an input of $n$ values, we sum the products of the *cost* and *times* columns, obtaining

$$
\begin{align}
T(n) = \  
& c_2 n + c_3(n-1) + c_5(n-1) + c_6 \sum_{i=2}^{n} t_i \\
& + c_7 \sum_{i=2}^{n} (t_i-1) + c_8 \sum_{i=2}^{n} (t_i-1) + c_{10}(n-1)
\end{align}
$$

Even for inputs of a given size, an algorithm’s running time may depend on
which input of that size is given.

<div class="alert-example">

In insertion-sort, the *best case* occurs when the array is already sorted.

- In this case, each time that line 5 executes, the value of key—the value originally in $A[i]$—is already greater than or equal to all values in $A[1:i-1]$, so that the while loop of lines 6-9 always exits upon the first test in line 6.
- Therefore, we have that <mark>$t_i = 1$ for $i = 2, 3, \dots, n$</mark>, and the best-case running time is given by

$$
\begin{align}
T(n)
& = c_2 n + c_3(n-1) + c_5(n-1) + c_6 \sum_{i=2}^{n} 1 + c_{10}(n-1) \\\\
& = c_2 n + c_3(n-1) + c_5(n-1) + c_6(n-1) + c_{10}(n-1) \\\\
& = (c_2 + c_3 + c_5 + c_6 + c_{10}) n - (c_3 + c_5 + c_6 + c_{10}) \\\\
& = a n + b
\end{align}
$$

$a$ and $b$ are *constants*. The running time is thus a **linear function** of $n$.

</div>

<div class="alert-example">

The *worst case* arises when the array is in reverse sorted order—that is, it starts out in decreasing order.

The procedure must compare each element $A[i]$ with each
element in the *entire* sorted subarray $A[1:i-1]$, and so <mark>$t_i = i$ for $i = 2, 3, \dots, n$</mark>.
(The procedure finds that $A[j] > key$ every time in line 6, and the $\text{while}$ loop exits
only when $j$ reaches 0.)

Noting that

$$
\begin{align}
\sum_{i=2}^{n} t_i \tag{$t_i = i$ for $i = 2, 3, \dots, n$}
& = \sum_{i=2}^{n} i \\
& = \Big( \sum_{i=1}^{n} i \Big) - 1 \\
& = \dfrac{n(n+1)}{2} - 1 \tag{by equation (A.2)}
\end{align}
$$

and

$$
\begin{align}
& \ \sum_{i=2}^{n} (t_i-1) \tag{$t_i = i$ for $i = 2, 3, \dots, n$} \\
= & \ \sum_{i=2}^{n} (i-1) = \sum_{j=1}^{n-1} j \tag{let $j = i-1$} \\
= & \ \dfrac{n(n-1)}{2} \tag{by equation (A.2)}
\end{align}
$$

we find that in the worst case, the running time of insertion-sort is

$$
\begin{align}
T(n)
& = c_2 n + c_3(n-1) + c_5(n-1) + c_6 \Big( \dfrac{n(n+1)}{2} - 1 \Big) \\
& \qquad + c_7 \Big( \dfrac{n(n-1)}{2} \Big) + c_8 \Big( \dfrac{n(n-1)}{2} \Big) + c_{10}(n-1) \\\\
& = a n^2 + b n + c
\end{align}
$$

The running time is thus a **quadratic function** of n.

</div>

Typically, as in insertion sort, the running time of an algorithm is fixed for a given input, although we’ll also see some interesting “randomized” algorithms whose behavior can vary even for a fixed input.

## Worst-Case and Average-Case Analysis

***Worst-Case***

We’ll usually (but not always) concentrate on finding only the **worst-case running time**, that is, the longest running time for any input of size $n$.

- The worst-case running time of an algorithm gives an *upper bound* on the running time for *any* input.
- For some algorithms, the worst case occurs fairly often.
- The “average case” is often roughly as bad as the worst case.

<div class="alert-example">

Suppose that you run insertion sort on an array of $n$ randomly chosen numbers. How long does it take to determine where in subarray $A[1:i-1]$ to insert element $A[i]$?

- On average, half the elements in $A[1:i-1]$ are less than $A[i]$, and half the elements are greater.
- On average, therefore, $A[i]$ is compared with just half of the subarray $A[1:i-1]$, and
- so $t_i$ is about $i/2$.

The resulting average-case running time turns out to be a *quadratic function* of the input size, just like the worst-case running time.

</div>

***Average-Case***

In some particular cases, we’ll be interested in the **average-case** running time of an algorithm.

We’ll see the technique of **probabilistic analysis** applied to various algorithms throughout this book.

- The scope of average-case analysis is limited, because it may not be apparent what constitutes an “average” input for a particular problem.

- Often, we’ll assume that all inputs of a given size are equally likely.

    In practice, this assumption may be violated, but we can sometimes use a **randomized algorithm**, which makes random choices, to allow a probabilistic analysis and yield an **expected** running time.

    We explore randomized algorithms more in Chapter 5 and in several other subsequent chapters.

## Order of growth

In order to ease our analysis of the insertion-sort procedure, we used some simplifying abstractions. We ignored not only the actual statement costs, but also the abstract costs. (用抽象成本 $c_k$ 來表示實際成本，最後再將抽象成本整理成 $a, b, c$ 常數。因此實際及抽象成本皆被我們忽略了。)

Let’s now make one more simplifying abstraction: it is the **rate of growth**, or **order of growth**, of the running time that really interests us.

- We therefore *consider* only the <mark>leading term of a formula</mark> (e.g., $an^2$ ), since the lower-order terms are relatively insignificant for large values of $n$.
- We also *ignore* the leading term’s constant coefficient (e.g., $a$), since constant factors are less significant than the rate of growth in determining computational efficiency for large inputs.

<div class="alert-example">

For insertion sort’s worst-case running time ($a n^2 + b n + c$), when we ignore the lower-order terms and the leading term’s constant coefficient, only the factor of $n^2$ from the leading term remains. That factor, $n^2$, is by far the most important part of the running time.

</div>

<div class="alert-example">

Suppose that an algorithm implemented on a particular machine takes $\dfrac{1}{100} n^2 + 100n + 17$ microseconds on an input of size $n$.

- Although the coefficients of $\dfrac{1}{100}$ for the $n^2$ term and $100$ for the $n$ term differ by four orders of magnitude, the $\dfrac{1}{100}n^2$ term dominates the $100n$ term once $n$ exceeds 10,000.
- Although 10,000 might seem large, it is smaller than the population of an average town. Many real-world problems have much larger input sizes.

</div>

***&Theta;-Notation***

To highlight the order of growth of the running time, we have a special notation
that uses the Greek letter &Theta; (theta).

- We write that insertion sort has a worst-case running time of $\Theta (n^2)$ (pronounced “theta of n-squared” or just “theta n-squared”).
- We also write that insertion sort has a best-case running time of  $\Theta (n)$ (“theta of n” or “theta n”).

---

For now, think of &Theta;-notation as saying “roughly proportional when $n$ is large,” so that

- $\Theta (n^2)$ means “roughly proportional to $n^2$ when $n$ is large”.
- $\Theta (n)$ means “roughly proportional to $n$ when $n$ is large”.

We’ll use &Theta;-notation informally in this chapter and define it precisely in Chapter 3.

---

We usually consider one algorithm to be more efficient than another if its worst-case running time has a lower order of growth.

- Due to constant factors and lower-order terms, an algorithm whose running time has a higher order of growth might take less time for *small* inputs than an algorithm whose running time has a lower order of growth.
- But on *large* enough inputs, an algorithm whose worst-case running time is $\Theta (n^2)$, for example, takes less time in the worst case than an algorithm whose worst-case running time is $\Theta (n^3)$.

Regardless of the constants hidden by the &Theta;-notation, there is always *some number*, say $n_0$, such that for all input sizes $n \geq n_0$, the $\Theta (n^2)$ algorithm beats the $\Theta (n^3)$ algorithm in the worst case.
