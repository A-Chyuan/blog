# Designing Algorithms

You can choose from a wide range of algorithm design techniques.

<div class="alert-example">

Insertion sort uses the **incremental** method: for each element $A[i]$, insert it into its proper place in the subarray $A[1:i]$, having already sorted the subarray $A[1:i-1]$.

</div>

This section examines another design method, known as “divide-and-conquer”, which we explore in more detail in Chapter 4.

- We’ll use divide-and-conquer to design a sorting algorithm whose worst-case running time is much less than that of insertion sort.
- One advantage of using an algorithm that follows the divide-and-conquer method is that analyzing its running time is often straightforward, using techniques that we’ll explore in Chapter 4.

## The Divide-And-Conquer Method

- Many useful algorithms are **recursive** in structure: to solve a given problem, they **recurse** (call themselves) one or more times to handle closely related subproblems.
- These algorithms typically follow the **divide-and-conquer** method: they break the problem into several subproblems that are similar to the original problem but smaller in size, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.

In the divide-and-conquer method:

<div class="stepper">

- If the problem is small enough—the **base case**—you just solve it directly without recursing.

- Otherwise—the recursive case—you perform three characteristic steps:

    - *Divide* the problem into one or more subproblems that are smaller instances of the same problem.
    - *Conquer* the subproblems by solving them recursively.
    - *Combine* the subproblem solutions to form a solution to the original problem.

</div>

### Merge Sort

The **merge sort** algorithm closely follows the divide-and-conquer method. In each step, it sorts a subarray $A[p:r]$, starting with the entire array $A[1:n]$ and recursing down to smaller and smaller subarrays. Here is how merge sort operates:

- Divide the subarray $A[p:r]$ to be sorted into two adjacent subarrays, each of half the size. To do so, compute the midpoint $q$ of $A[p:r]$ (taking the average of $p$ and $r$ ), and divide $A[p:r]$ into subarrays $A[p:q]$ and $A[q+1:r]$.
- Conquer by sorting each of the two subarrays $A[p:q]$ and $A[q+1:r]$ recursively using merge sort.
- Combine by merging the two sorted subarrays $A[p:q]$ and $A[q+1:r]$ back into $A[p:r]$, producing the sorted answer.

The recursion “bottoms out”—it reaches the *base case*—when the subarray $A[p:r]$ to be sorted has just 1 element, that is, when $p$ equals $r$. As we noted in the initialization argument for insertion-sort’s loop invariant, <mark>a subarray comprising just a single element is always sorted.</mark>

***Merge Procedure***

The key operation of the merge sort algorithm occurs in the “combine” step, which merges two adjacent, sorted subarrays.

The merge operation is performed by the auxiliary procedure $\text{merge}(A, p, q, r)$:

- $A$ is an array and $p$, $q$, and $r$ are indices into the array such that $p \leq q < r$ .
- The procedure assumes that the adjacent subarrays $A[p:q]$ and $A[q+1:r]$ were already recursively sorted.
- It **merges** the two sorted subarrays to form a single sorted subarray that replaces the current subarray $A[p:r]$.

In detail, the merge procedure works as follows. It copies the two subarrays $A[p:q]$ and $A[q+1:r]$ into temporary arrays $L$ and $R$ (“left” and “right”), and then it merges the values in $L$ and $R$ back into $A[p:r]$.

- Lines 2 and 3 compute the lengths $n_L$ and $n_R$ of the subarrays $A[p:q]$ and $A[q+1:r]$, respectively.
- Line 4 creates arrays $L[0:n_L-1]$ and $R[0:n_R-1]$ with respective lengths $n_L$ and $n_R$.
- The for loop of lines 6-8 copies the subarray $A[p:q]$ into $L$.
- The for loop of lines 10-12 copies the subarray $A[q+1:r]$ into $R$.

```algorithm
\begin{algorithm}
\begin{algorithmic}

\procedure{merge}{A, p, q, r}

\state $n_L$ = q $-$ p + 1    $\hspace{2em}$ \comment{length of A[p : q]}
\state $n_R$ = r $-$ q        $\hspace{4em}$ \comment{length of A[q+1 : r]}
\state let L[0 : $n_L-1$] and R[0 : $n_R-1$] be new arrays

\state \comment{copy A[p : q] into L[0 : $n_L-1$]}
\for{i = 0 to $n_L-1$}
    \state L[i] = A[p + i]
\endFor

\state \comment{copy A[q+1 : r] into R[0 : $n_R-1$]}
\for{j = 0 to $n_R-1$}
    \state R[j] = A[q + j + 1]
\endFor

\state i = 0       $\hspace{2em}$ \comment{i indexes the smallest remaining element in L}
\state j = 0       $\hspace{2em}$ \comment{j indexes the smallest remaining element in R}
\state k = p       $\hspace{1.7em}$ \comment{k indexes the location in A to fill}

\state \comment{As long as each of the arrays L and R contains an unmerged element,}
\state \comment{copy the smallest unmerged element back into A[p : r].}

\while{i < $n_L$ and j < $n_R$}
    \if{L[i] $\leq$ R[j]}
        \state A[k] = L[i]
        \state i = i + 1
    \else
        \state A[k] = R[j]
        \state j = j + 1
    \endIf
    \state k = k + 1
\endWhile

\state \comment{Having gone through one of L and R entirely, copy the}
\state \comment{remainder of the other to the end of A[p : r].}

\while{i < $n_L$}
    \state A[k] = L[i]
    \state i = i + 1
    \state k = k + 1
\endWhile

\while{j < $n_R$}
    \state A[k] = R[j]
    \state j = j + 1
    \state k = k + 1
\endWhile

\endProcedure

\end{algorithmic}
\end{algorithm}
```

Lines 13-27, illustrated in Figure 2.3, perform the basic steps.

- The while loop of lines 18-27 repeatedly identifies the smallest value in $L$ and $R$ that has yet to be copied back into $A[p:r]$ and copies it back in. As the comments indicate, the index $k$ gives the position of $A$ that is being filled in, and the indices $i$ and $j$ give the positions in $L$ and $R$, respectively, of the smallest remaining values.

- Eventually, either all of $L$ or all of $R$ is copied back into $A[p:r]$, and this loop terminates.

    - If the loop terminates because all of $R$ has been copied back, that is, because $j$ equals $n_R$ , then $i$ is still less than $n_L$ , so that some of $L$ has yet to be copied back, and these values are the greatest in both $L$ and $R$.

    In this case, the while loop of lines 30-34 copies these remaining values of $L$ into the last few positions of $A[p:r]$. Because $j$ equals $n_R$ , the while loop of lines 35-39 iterates 0 times.

    - If instead the while loop of lines 18-27 terminates because $i$ equals $n_L$ , then all of $L$ has already been copied back into $A[p:r]$, and the while loop of lines 35-39 copies the remaining values of $R$ back into the end of $A[p:r]$.

![](./img/2.3.png ':figure')

To see that the merge procedure runs in $\Theta (n)$ time,

- where $n = r - p + 1$, observe that each of lines 2-4 and 13-15 takes constant time, and the for loops
of lines 6-12 take $\Theta(n_L + n_R) = \Theta (n)$ time.
- To account for the three while loops of lines 18-27, 30-34, and 35-39, observe that each iteration of these loops copies exactly one value from $L$ or $R$ back into $A$ and that every value is copied back into $A$ exactly once. Therefore, these three loops together make a total of $n$ iterations. Since each iteration of each of the three loops takes constant time, the total time spent in these three loops is $\Theta (n)$.

***Merge-Sort Procedure***

We can now use the merge procedure as a subroutine in the merge sort algorithm.

The procedure $\text{merge-sort}(A, p, r)$ sorts the elements in the subarray $A[p:r]$.

- If $p$ equals $r$ , the subarray has just 1 element and is therefore already sorted.
- Otherwise, we must have $p < r$ , and merge-sort runs the divide, conquer, and combine steps.
- The divide step simply computes an index $q$ that partitions $A[p:r]$ into two adjacent subarrays: $A[p:q]$, containing $\lceil n/2 \rceil$ elements, and $A[q+1:r]$, containing $\lfloor n/2 \rfloor$ elements.
- The initial call $\text{merge-sort}(A, 1, n)$ sorts the entire array $A[1:n]$.

![](./img/2.4.png ':figure')

Figure 2.4 illustrates the operation of the procedure for $n = 8$, showing also the sequence of divide and merge steps.

- The algorithm recursively divides the array down to 1-element subarrays.
- The combine steps merge pairs of 1-element subarrays to form sorted subarrays of length 2, merges those to form sorted subarrays of length 4, and merges those to form the final sorted subarray of length 8.
- If $n$ is not an exact power of 2, then some divide steps create subarrays whose lengths differ by 1. (For example, when dividing a subarray of length 7, one subarray has length 4 and the other has length 3.)
- Regardless of the lengths of the two subarrays being merged, the time to merge a total of $n$ items is $\Theta (n)$.

```algorithm
\begin{algorithm}
\begin{algorithmic}

\procedure{merge-sort}{A, p, r}

\state \comment{zero or one element?}
\if{p $\geq$ r}
    \return
\endIf

\state q = $\lfloor$ (p + r)/2 $\rfloor$    $\hspace{4em}$ \comment{midpoint of A[p : r]}
\state \call{merge-sort}{A, p, q}           $\hspace{3em}$ \comment{recursively sort A[p : q]}
\state \call{merge-sort}{A, q+1, r}         $\hspace{1.8em}$ \comment{recursively sort A[q + 1 : r]}

\state \comment{Merge A[p : q] and A[q + 1 : r] into A[p : r].}
\state \call{merge}{A, p, q, r}

\endProcedure

\end{algorithmic}
\end{algorithm}
```

## Analyzing Divide-And-Conquer Algorithms

When an algorithm contains a recursive call, you can often describe its running time by a **recurrence equation** or **recurrence**, which describes the overall running time on a problem of size $n$ in terms of the running time of the same algorithm on smaller inputs. You can then use mathematical tools to solve the recurrence and provide bounds on the performance of the algorithm.

<div class="alert-note">

A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic method.

- As we did for insertion sort, let $T(n)$ be the worst-case running time on a problem of size $n$. If the problem size is small enough, say $n < n_0$ for some constant $n_0 > 0$, the straightforward solution takes constant time, which we write as $\Theta (1)$.

- Suppose that the division of the problem yields $a$ subproblems, each with size $\dfrac{n}{b}$, that is, $\dfrac{1}{b}$ the size of the original.

    - For merge sort, both $a$ and $b$ are 2,
    - but we’ll see other divide-and-conquer algorithms in which $a \neq b$. It takes $T(\dfrac{n}{b})$ time to solve one subproblem of size $\dfrac{n}{b}$, and so it takes $a \cdot T(\dfrac{n}{b})$ time to solve all $a$ of them.

- If it takes $D(n)$ time to divide the problem into subproblems and $C(n)$ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence

$$
T(n) =
\begin{cases}
& \Theta(1) & \text{if $n < n_0$} \\\\
& D(n) + a T(n/b) + C(n) & \text{otherwise}
\end{cases}
$$

Chapter 4 shows how to solve common recurrences of this form.

</div>

Sometimes, the $\dfrac{n}{b}$ size of the divide step isn’t an integer.

<div class="alert-example">

The merge-sort procedure divides a problem of size $n$ into subproblems of sizes $\lceil \dfrac{n}{2} \rceil$ and $\lfloor \dfrac{n}{2} \rfloor$.

- Since the difference between $\lceil \dfrac{n}{2} \rceil$ and $\lfloor \dfrac{n}{2} \rfloor$ is at most 1, which for large $n$ is much smaller than the effect of dividing $n$ by 2, we’ll squint a little and just call them both size $\dfrac{n}{2}$.
- As Chapter 4 will discuss, this simplification of ignoring floors and ceilings does not generally affect the order of growth of a solution to a divide-and-conquer recurrence.

</div>

Another convention we’ll adopt is to *omit* a statement of the base cases of the recurrence, which we’ll also discuss in more detail in Chapter 4.

- The reason is that the base cases are pretty much always $T(n) = \Theta (1)$ if $n < n_0$ for some constant $n_0 > 0$.
- That’s because the running time of an algorithm on an input of constant size is constant. We save ourselves a lot of extra writing by adopting this convention.

### Analysis of Merge Sort

Here’s how to set up the recurrence for $T(n)$, the worst-case running time of merge sort on $n$ numbers.

***Divide***

The divide step just computes the middle of the subarray, which takes constant time. Thus, $D(n) = \Theta (1)$.

***Conquer***

Recursively solving two subproblems, each of size $\dfrac{n}{2}$, contributes $2 \cdot T(\dfrac{n}{2})$ to the running time (ignoring the floors and ceilings, as we discussed).

***Combine***

Since the merge procedure on an n-element subarray takes $\Theta (n)$ time, we have $C(n) = \Theta (n)$.

***Compute the $T(n)$***

When we add the functions $D(n)$ and $C(n)$ for the merge sort analysis, we are adding a function that is $\Theta (n)$ and a function that is $\Theta (1)$. This sum is a linear function of $n$. That is, it is roughly proportional to $n$ when $n$ is large, and so merge sort’s dividing and combining times together are $\Theta (n)$.

Adding $\Theta (n)$ to the $2 \cdot T(\dfrac{n}{2})$ term from the conquer step gives the recurrence for the worst-case running time $T(n)$ of merge sort:

$$
T(n) = 2 \cdot T(\dfrac{n}{2}) + \Theta(n) \tag{2.3}
$$

---

Chapter 4 presents the **master theorem**, which shows that $T(n) = \Theta (n \log n)$.

- The logarithm function grows more slowly than any linear function.
- For large enough inputs, merge sort, with its $\Theta (n \log n)$ worst-case running time, outperforms insertion sort, whose worst-case running time is $\Theta (n^2)$.

---

We do *not need* the master theorem, however, to understand intuitively why the solution to recurrence (2.3) is $T(n) = \Theta (n \log n)$. For simplicity, assume that $n$ is an exact power of 2 and that the implicit base case is $n = 1$. Then recurrence (2.3) is essentially

$$
T(n) =
\begin{cases}
& c_1 & \text{if } n = 1 \\\\
& 2 \cdot T(\dfrac{n}{2}) + c_2 n & \text{if } n > 1
\end{cases}
\tag{2.4}
$$

where the constant $c_1 > 0$ represents the time required to solve a problem of size 1, and $c_2 > 0$ is the time per array element of the divide and combine steps.

Figure 2.5 illustrates one way of figuring out the solution to recurrence (2.4).

!> 圖有錯誤，應為：「Total: $c_2 n \cdot \log n + c_1 n$」

![](./img/2.5.png ':figure')

- Part (a) of the figure shows $T(n)$
- Part (b) expands into an equivalent tree representing the recurrence. The $c_2 n$ term denotes the cost of dividing and combining at the top level of recursion, and the two subtrees of the root are the two smaller recurrences $T(\dfrac{n}{2})$.
- Part (c) shows this process carried one step further by expanding $T(\dfrac{n}{2})$. The cost for dividing and combining at each of the two nodes at the second level of recursion is $c_2 \dfrac{n}{2}$.
- Continue to expand each node in the tree by breaking it into its constituent parts as determined by the recurrence, until the problem sizes get down to 1, each with a cost of $c_1$.
- Part (d) shows the resulting **recursion tree**.

In general,

- the level that is $i$ levels below the top has $2^i$ nodes, each contributing a cost of $c_2 \cdot \dfrac{n}{2^i}$, so that the $i$ th level below the top has total cost $2^i \cdot (c_2  \cdot \dfrac{n}{2^i}) = c_2 n$.
- The bottom level has $n$ nodes, each contributing a cost of $c_1$, for a total cost of $c_1 n$.

The total number of levels of the recursion tree in Figure 2.5 is $\log n + 1$, where $n$ is the number of leaves, corresponding to the input size.

- The fully expanded tree in part (d) has $\log n + 1$ levels.
- Each level above the leaves contributes a total cost of $c_2 n$.
- The leaf level contributes $c_1 n$.
- The total cost, therefore, is

$$
c_2 n \cdot \log n + c_1 n = \Theta (n \log n)
$$
