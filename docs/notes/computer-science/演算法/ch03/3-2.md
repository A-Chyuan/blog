# Asymptotic Notation: Formal Definitions

Graphic examples of the O, &Omega;, and &Theta; notations. In each part, the value of $n_0$ shown is the minimum possible value, but any greater value also works.

![](./img/3.2.png ':figure')

- O-notation gives an upper bound for a function to within a constant factor. We write $f(n) = O(g(n))$ if there are positive constants $n_0$ and $c$ such that at and to the right of $n_0$, the value of $f(n)$ always lies on or below $cg(n)$.
- &Omega;-notation gives a lower bound for a function to within a const ant factor. We write $f(n) = \Omega(g(n))$ if there are positive constants $n_0$ and $c$ such that at and to the right of $n_0$, the value of $f(n)$ always lies on or above $cg(n)$.
- &Theta;-notation bounds a function to within constant factors. We write $f(n) = \Theta(g(n))$ if there exist positive constants $n_0$, $c_1$, and $c_2$ such that at and to the right of $n_0$, the value of $f(n)$ always lies between $c_1g(n)$ and $c_2g(n)$ inclusive.

## O-notation (Big-Oh)

Here is the formal definition of O-notation.

<div class="alert-note">

For a given function $g(n)$, we denote by $O(g(n))$ the set of functions

$$
\begin{align}
O(g(n)) =
\{
f(n):
& \text{ there exist positive constants $c$ and $n_0$ such that } \\
& \text{ $0 \leq f(n) \leq cg(n)$ for all $n \geq n_0$ }
\}
\end{align}
$$

</div>

A function $f(n)$ belongs to the set $O(g(n))$ if there exists a positive constant $c$ such that $f(n) \leq cg(n)$ for sufficiently large $n$. Figure 3.2(a) shows the intuition behind O-notation. For all values $n$ at and to the right of $n_0$, the value of the function $f(n)$ is on or *below* $cg(n)$.

![](./img/3.2.png ':figure')

***Asymptotically Nonnegative***

This assumption holds for the other asymptotic notations defined in this chapter as well.

The definition of $O(g(n))$ requires that every function $f(n)$ in the set $O(g(n))$ be **asymptotically nonnegative**: $f(n)$ must be nonnegative whenever $n$ is sufficiently large. (An **asymptotically positive** function is one that is positive for all sufficiently large $n$.)

Consequently, the function $g(n)$ itself must be asymptotically nonnegative, or else the set $O(g(n))$ is empty. We therefore assume that every function used within O-notation is asymptotically nonnegative.

$g(n)$ 為負數就無法滿足定義中的不等式了，使得 $O(g(n))$ 為空集合。

***Use &isinv; or =***

You might be surprised that we define O-notation in terms of sets.

- Indeed, you might expect that we would write “$f(n) \in O(g(n))$” to indicate that $f(n)$ belongs to the set $O(g(n))$.
- Instead, we usually write “$f(n) = O(g(n))$” and say “$f(n)$ is big-oh of $g(n)$” to express the same notion.

Although it may seem confusing at first to abuse equality in this way, we’ll see [later][] in this section that doing so has its advantages.

[later]: /notes/computer-science/演算法/ch03/3-2?id=asymptotic-notation-in-equations-and-inequalities

***Example***

Let’s explore an example of how to use the formal definition of O-notation to justify our practice of discarding lower-order terms and ignoring the constant coefficient of the highest-order term.

<div class="alert-example">

We’ll show that $4n^2 + 100n + 500 = O(n^2)$, even though the lower-order terms have much larger coefficients than the leading term.

We need to find positive constants $c$ and $n_0$ such that $4n^2 + 100n + 500 \leq cn^2$ for all $n \geq n_0$.

Dividing both sides by $n^2$ gives $4 + \dfrac{100}{n} + \dfrac{500}{n^2} \leq c$. This inequality is satisfied for many choices of $c$ and $n_0$. For example:

- If we choose $n_0 = 1$, then this inequality holds for $c = 604$.
- If we choose $n_0 = 10$, then $c = 19$ works,
- and choosing $n_0 = 100$ allows us to use $c = 5.05$.

當 $n$ 越大，$\dfrac{100}{n}$ 以及 $\dfrac{500}{n^2}$ 越趨近於 0，可見低階項對成長率的影響並不重要。而 $4n^2$ 只是 $n^2$ 的常數倍數，總是可以找到較大的 c 來吸收這個常數係數 4，因此可以被忽略。

</div>

<div class="alert-example">

We can also use the formal definition of O-notation to show that the function $n^3 - 100n^2$ does not belong to the set $O(n^2)$, even though the coefficient of $n^2$ is a large negative number.

If we had $n^3 - 100n^2 = O(n^2)$, then there would be positive constants $c$ and $n_0$ such that $n^3 - 100n^2 \leq cn^2$ for all $n \geq n_0$.

- Again, we divide both sides by $n^2$, giving $n - 100 \leq c$.
- Regardless of what value we choose for the constant $c$, this inequality does not hold for any value of $n > c + 100$.

因為我們無法找到一個適當的常數 $c$ 使得不等式對所有 $n \geq n_0$ 成立，這意味著：

$$
n^3 - 100n^2 \neq O(n^2)
$$

</div>

## &Omega;-Notation (Big-Omega)

<div class="alert-note">

For a given function $g(n)$, we denote by $\Omega(g(n))$

$$
\begin{align}
\Omega(g(n)) =
\{
f(n):
& \text{ there exist positive constants $c$ and $n_0$ such that } \\
& \text{ $0 \leq cg(n) \leq f(n)$ for all $n \geq n_0$ }
\}
\end{align}
$$

</div>

Figure 3.2(b) shows the intuition behind &Omega;-notation. For all values $n$ at or to the right of $n_0$, the value of $f(n)$ is on or *above* $cg(n)$.

![](./img/3.2.png ':figure')

<div class="alert-example">

Now let’s show that $4n^2 + 100n + 500 = \Omega(n^2)$.

We need to find positive constants $c$ and $n_0$ such that $4n^2 + 100n + 500 \geq cn^2$ for all $n \geq n_0$.

- As before, we divide both sides by $n^2$ giving $4 + \dfrac{100}{n} + \dfrac{500}{n^2} \geq c$.
- This inequality holds when $n_0$ is any positive integer and $c = 4$.

</div>

What if we had subtracted the lower-order terms from the $4n^2$ term instead of adding them? What if we had a small coefficient for the $n^2$ term? The function would still be $\Omega(n^2)$.

<div class="alert-example">

Let’s show that $\dfrac{n^2}{100} - 100n - 500 = \Omega(n^2)$.

Dividing by $n^2$ gives $\dfrac{1}{100} - \dfrac{100}{n} - \dfrac{500}{n^2} \geq c$. We can choose any value for $n_0$ that is at least 10,005 and find a positive value for $c$. For example:

- When $n_0 = 10,005$, we can choose $c = 2.49 \times 10^{-9}$. Yes, that’s a tiny value for $c$, but it is positive. If we select a larger value for $n_0$, we can also increase $c$.
- If $n_0 = 100,000$, then we can choose $c = 0.0089$. The higher the value of $n_0$, the closer to the coefficient $\dfrac{1}{100}$ we can choose $c$.

</div>

## &Theta;-Notation (Theta)

<div class="alert-note">

For a given function $g(n)$, we denote by $\Theta(g(n))$ the set of functions

$$
\begin{align}
\Theta(g(n)) =
\{
f(n):
& \text{ there exist positive constants $c_1$, $c_2$, and $n_0$ such that} \\
& \text{ $0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n)$ for all $n \geq n_0$}
\}
\end{align}
$$

</div>

Figure 3.2(c) shows the intuition behind &Theta;-notation. For all values of $n$ at and to the right of $n_0$, the value of $f(n)$ lies at or above $c_1 g(n)$ and at or below $c_2 g(n)$. In other words, for all $n \geq n_0$, the function $f(n)$ is equal to $g(n)$ to within constant factors.

![](./img/3.2.png ':figure')

The definitions of O-, &Omega;-, and &Theta;-notations lead to the following theorem, whose
proof we leave as Exercise 3.2-4.

***Theorem 3.1***

For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.

## Asymptotic Notation and Running Times

在使用漸近符號來描述演算法的執行時間時，應確保所使用的符號既精確又不會過度描述時間複雜度。以下是一些正確與錯誤使用漸近符號的例子：

***Insertion Sort***

We can correctly say that insertion sort’s <mark>worst-case</mark> running time is $O(n^2)$, $\Omega(n^2)$, and—due to Theorem 3.1—$\Theta(n^2)$. Although all three ways to characterize the worst-case running times are correct, the $\Theta(n^2)$ bound is the most precise and hence the most preferred.

We can also correctly say that insertion sort’s <mark>best-case</mark> running time is $O(n)$, $\Omega(n)$, and $\Theta(n)$, again with $\Theta(n)$ the most precise and therefore the most preferred.

Here is what we *cannot* correctly say: insertion sort’s running time is $\Theta(n^2)$.

- The error here is that insertion sort does not run in $\Theta(n^2)$ time in all cases since, as we’ve seen, it runs in $\Theta(n)$ time in the best case.

We can correctly say that insertion sort’s running time is $O(n^2)$, however, because in all cases, its running time grows no faster than $n^2$. When we say $O(n^2)$ instead of $\Theta(n^2)$, there is no problem in having cases whose running time grows more slowly than $n^2$.

Likewise, we *cannot* correctly say that insertion sort’s running time is $\Theta(n)$, *but* we can say that its running time is $\Omega(n)$.

***Merge Sort***

Since merge sort runs in $\Theta(n \log n)$ time in *all* cases, we can just say that its running time is $\Theta(n \log n)$ without specifying worst-case, best-case, or any other case.

***Mistakenly Using O-notation***

People occasionally conflate O-notation with &Theta;-notation by mistakenly using O-notation to indicate an asymptotically tight bound.

<div class="alert-example">

They say things like “an $O(n \log n)$-time algorithm runs faster than an $O(n^2)$-time algorithm”.

Maybe it does, maybe it doesn’t. Since O-notation denotes only an asymptotic upper bound, that so-called $O(n^2)$-time algorithm might actually run in $\Theta(n)$ time.

You should be careful to choose the appropriate asymptotic notation. If you want to indicate an asymptotically tight bound, use &Theta;-notation.

</div>

***Typical Usage***

We typically use asymptotic notation to provide the simplest and most precise bounds possible.

<div class="alert-example">

If an algorithm has a running time of $3n^2 + 20n$ in all cases, we use asymptotic notation to write that its running time is $\Theta(n^2)$.

Strictly speaking, we are also correct in writing that the running time is $O(n^3)$ or $\Theta(3n^2 + 20n)$. Neither of these expressions is as useful as writing $\Theta(n^2)$ in this case, however:

- $O(n^3)$ is less precise than $\Theta(n^2)$ if the running time is $3n^2 + 20n$, and
- $\Theta(3n^2 + 20n)$ introduces complexity that obscures the order of growth.

By writing the simplest and most precise bound, such as $\Theta(n^2)$, we can categorize and compare different algorithms.

</div>

Throughout the book, you will see asymptotic running times that are almost always based on polynomials and logarithms:

- functions such as $n$, $n \log^2 n$, $n^2 \log n$, or $n^{1/2}$.
- You will also see some other functions, such as exponentials, $\log \log n$, and $\log^* n$ (see [3-3][]).

It is usually fairly easy to compare the rates of growth of these functions.

[3-3]: /notes/computer-science/演算法/ch03/3-3

## Asymptotic Notation in Equations and Inequalities

Although we formally define asymptotic notation in terms of sets, we use the equal sign (=) instead of the set membership sign (&isinv;) within formulas.

<div class="alert-example">

We write:

- $4n^2 + 100n + 500 = O(n^2)$
- $2n^2 + 3n + 1 = 2n^2 + \Theta(n)$

</div>

How do we interpret such formulas?

When the asymptotic notation stands alone (that is, not within a larger formula) on the right-hand side of an equation (or inequality), as in $4n^2 + 100n + 500 = O(n^2)$, the equal sign means set membership: $4n^2 + 100n + 500 \in O(n^2)$.

In general, however, when asymptotic notation appears in a formula, we interpret it as standing for some anonymous function that we do not care to name.

<div class="alert-example">

The formula $2n^2 + 3n + 1 = 2n^2 + \Theta(n)$ means that $2n^2 + 3n + 1 = 2n^2 + f(n)$, where $f(n) \in \Theta(n)$. In this case, we let $f(n) = 3n + 1$, which indeed belongs to $\Theta(n)$.

</div>

Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation.

<div class="alert-example">

In Chapter 2 we expressed the worst-case running time of merge sort as the recurrence

$$
T(n) = 2T(\dfrac{n}{2}) + \Theta(n)
$$

If we are interested only in the asymptotic behavior of $T(n)$, there is no point in specifying all the lower-order terms exactly, because they are all understood to be included in the anonymous function denoted by the term $\Theta(n)$.

</div>

The number of anonymous functions in an expression is understood to be equal to the number of times the asymptotic notation appears.

<div class="alert-example">

In the expression

$$
\sum_{i=1}^{n} O(i)
$$

there is only a single anonymous function (a function of $i$). This expression is thus *not* the same as $O(1) + O(2) + \dots + O(n)$, which doesn’t really have a clean interpretation.

</div>

In some cases, asymptotic notation appears on the left-hand side of an equation, as in

$$
2n^2 + \Theta(n) = \Theta(n^2)
$$

Interpret such equations using the following rule: <mark>No matter how the anonymous functions are chosen on the left of the equal sign, there is a way to choose the anonymous functions on the right of the equal sign to make the equation valid.</mark>

<div class="alert-example">

Thus, our example

$$
2n^2 + \Theta(n) = \Theta(n^2)
$$

means that for any function $f(n) \in \Theta(n)$, there is some function $g(n) \in \Theta(n^2)$ such that $2n^2 + f(n) = g(n)$ for all $n$. In other words, the right-hand side of an equation provides a coarser level of detail than the left-hand side.

</div>

We can chain together a number of such relationships.

<div class="alert-example">

$$
\begin{align}
2n^2 + 3n + 1
& = 2n^2 + \Theta(n) \\
& = \Theta(n^2)
\end{align}
$$

By the rules above, interpret each equation separately.

- The first equation says that there is *some* function $f(n) \in \Theta(n)$ such that $2n^2 + 3n + 1 = 2n^2 + f(n)$ for all $n$.
- The second equation says that for any function $g(n) \in \Theta(n)$ (such as the $f(n)$ just mentioned), there is some function $h(n) \in \Theta(n^2)$ such that $2n^2 + g(n) = h(n)$ for all $n$.
- This interpretation implies that $2n^2 + 3n + 1 = \Theta(n^2)$, which is what the chaining of equations intuitively says.

</div>

## o-notation (Little-Oh)

The asymptotic upper bound provided by O-notation may or may not be asymptotically tight.

- The bound $2n^2 = O(n^2)$ is asymptotically tight,
- but the bound $2n = O(n^2)$ is not.

We use o-notation to denote an upper bound that is *not* asymptotically tight. We formally define $o(g(n))$ as the set

$$
\begin{align}
o(g(n)) =
\{
f(n):
& \text{ for any positive constant c > 0, there exists a constant} \\
& \text{ $n_0 > 0$ such that $0 \leq f(n) < cg(n)$ for all $n \geq n_0$}
\}
\end{align}
$$

不像大O，$f(n)$ 不能與 $g(n)$ 成長速率相同，它必須明顯小於 $g(n)$。

<div class="alert-example">

$2n = o(n^2)$, but $2n^2 \neq o(n^2)$.

</div>

The definitions of O-notation and o-notation are similar. The main difference is that

- in $f(n) = O(g(n))$, the bound $0 \leq f(n) \leq cg(n)$ holds for *some* constant $c > 0$, but
- in $f(n) = o(g(n))$, the bound $0 \leq f(n) < cg(n)$ holds for *all* constants $c > 0$.

Intuitively, in o-notation, the function $f(n)$ becomes insignificant relative to $g(n)$ as $n$ gets large:

$$
\lim_{n \to \infty} \dfrac{f(n)}{g(n)} = 0
$$

Some authors use this limit as a definition of the o-notation, but the definition in this book also restricts the anonymous functions to be asymptotically nonnegative.

## &omega;-notation (Little-Omega)

By analogy, &omega;-notation is to &Omega;-notation as o-notation is to O-notation. We use &omega;-notation to denote a lower bound that is not asymptotically tight.

One way to define it is by

$$
f(n) \in \omega(g(n)) \text{ if and only if } g(n) \in o(f(n))
$$

Formally, however, we define $\omega(g(n))$ as the set

$$
\begin{align}
\omega(g(n)) =
\{
f(n):
& \text{ for any positive constant c > 0, there exists a constant} \\
& \text{ $n_0 > 0$ such that $0 \leq cg(n) < f(n)$ for all $n \geq n_0$}
\}
\end{align}
$$

- Where the definition of o-notation says that $f(n) < cg(n)$,
- the definition of &omega;-notation says the opposite: that $cg(n) < f(n)$.

<div class="alert-example">

We have $\dfrac{n^2}{2} = \omega(n)$, but $\dfrac{n^2}{2} \neq \omega(n^2)$.

</div>

The relation $f(n) = \omega(g(n))$ implies that

$$
\lim_{n \to \infty} \dfrac{f(n)}{g(n)} = \infty
$$

if the limit exists. That is, $f(n)$ becomes arbitrarily large relative to $g(n)$ as $n$ gets large.
